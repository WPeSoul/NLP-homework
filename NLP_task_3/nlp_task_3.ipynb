{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи Нужно реализовать простейшую семантическую поисковую систему помощью векторного представления предложений/текстов.\n",
    "\n",
    "## 1. Загрузка и предварительная обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T00:25:34.417350900Z",
     "start_time": "2024-12-31T00:25:33.442130200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Парусная гонка Giraglia Rolex Cup пройдет в Ср...   \n",
      "1  Шведский хоккеист Матс Сундин назначен советни...   \n",
      "2  Гран-при конкурса \"Брэнд года/EFFIE\" получил г...   \n",
      "3  Цена американской нефти WTI на лондонской бирж...   \n",
      "4  Сбербанк выставил на продажу долги по 21,4 тыс...   \n",
      "\n",
      "                                preprocessed_content  \n",
      "0  [парусный, гонка,                    , пройти,...  \n",
      "1  [шведский, хоккеист, матс, сундин, назначить, ...  \n",
      "2  [гран, конкурса,  , брэнд, год,        , получ...  \n",
      "3  [цена, американский, нефть,     , лондонский, ...  \n",
      "4  [сбербанк, выставить, продажа, долг,      , ты...  \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import pickle  # 用于保存和加载DataFrame\n",
    "\n",
    "# 加载并解压数据 (Load and decompress data)\n",
    "def load_data_from_gz(gz_path):\n",
    "    with gzip.open(gz_path, 'rt', encoding='utf-8') as gz_file:\n",
    "        file_content = gz_file.read().strip()\n",
    "    data = [line.split('\\t') for line in file_content.splitlines() if len(line.split('\\t')) == 3]\n",
    "    # 将文件内容按行分割，并确保每行有三个部分（类别、标题、内容）\n",
    "    # Split the file content by lines and ensure each line has three parts (category, title, content)\n",
    "    df = pd.DataFrame(data, columns=['category', 'title', 'content'])\n",
    "    return df\n",
    "\n",
    "df = load_data_from_gz(r\"D:\\MyProject\\SPBU Course\\NLP\\nlp_task_3\\news.txt.gz\")\n",
    "\n",
    "# 加载spaCy的俄语模型 (Load the Russian model of spaCy)\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# 预处理函数：词形还原和去除停用词 (Preprocessing function: lemmatization and stopword removal)\n",
    "def preprocess(text, stopwords=None):\n",
    "    text_cleaned = re.sub(r'[^а-яА-Я]', ' ', text.lower())\n",
    "    doc = nlp(text_cleaned)\n",
    "    lemmatized_words = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and not token.is_punct\n",
    "    ]\n",
    "    if stopwords:\n",
    "        lemmatized_words = [word for word in lemmatized_words if word not in stopwords]\n",
    "    return lemmatized_words\n",
    "\n",
    "# 定义俄语停用列表 (Define a set of Russian stopwords)\n",
    "russian_stopwords = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она',\n",
    "                         'так', 'его', 'но', 'для', 'около', 'же', 'теперь', 'быть', 'бывать', 'этот', 'вот',\n",
    "                         'чем', 'еще', 'мочь', 'тот', 'когда', 'другой', 'первыи', 'ж', 'там', 'себя'])\n",
    "\n",
    "df['preprocessed_content'] = df['content'].apply(lambda x: preprocess(x, russian_stopwords))\n",
    "\n",
    "# 保存预处理后的DataFrame到本地\n",
    "with open('preprocessed_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "# 查看结果\n",
    "print(df[['content', 'preprocessed_content']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Выберите модель SentenceTransformer\n",
    "#### В частности, использовалась предварительно обученная модель 'paraphrase-MiniLM-L6-v2'. Эта модель способна преобразовывать текстовые предложения в высокоразмерные векторы (эмбеддинги), которые могут быть использованы для вычисления сходства между предложениями.\n",
    "## 3. Выбор FAISS в качестве хранилища векторов\n",
    "#### FAISS - это эффективная библиотека, предназначенная для поиска сходства в больших векторных коллекциях. Она поддерживает широкий спектр типов индексов и работает как на GPU, так и на CPU, обеспечивая быстрый поиск ближайших соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Creating embeddings...\n",
      "Embeddings created and saved.\n",
      "Initializing FAISS index...\n",
      "FAISS index initialized and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# 定义路径\n",
    "df_path = 'preprocessed_df.pkl'\n",
    "embeddings_path = 'all_embeddings.npy'\n",
    "index_path = 'faiss_index.index'\n",
    "\n",
    "def create_and_save_data():\n",
    "    # 加载预处理后的DataFrame (Load the preprocessed DataFrame)\n",
    "    with open(df_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    # 加载SentenceTransformer模型 (Load the SentenceTransformer model)\n",
    "    embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # 创建整个数据集的句子嵌入（如果不存在）(Create sentence embeddings for the entire dataset if they do not exist)\n",
    "    if not os.path.exists(embeddings_path):\n",
    "        print(\"Creating embeddings...\")\n",
    "        # 将列表中的单词拼接成句子 (Join words in the list into sentences)\n",
    "        all_embeddings = embedder.encode(df['preprocessed_content'].apply(lambda x: ' '.join(x)).tolist(), convert_to_tensor=True).cpu().numpy()\n",
    "        np.save(embeddings_path, all_embeddings)\n",
    "        print(\"Embeddings created and saved.\")\n",
    "    else:\n",
    "        all_embeddings = np.load(embeddings_path)\n",
    "\n",
    "    # 初始化FAISS索引（如果不存在）(Initialize the FAISS index if it does not exist)\n",
    "    if not os.path.exists(index_path):\n",
    "        print(\"Initializing FAISS index...\")\n",
    "        dimension = all_embeddings.shape[1]  # 获取嵌入维度 (Get the embedding dimension)\n",
    "        index = faiss.IndexFlatL2(dimension)  # 使用L2距离的平面索引 (Use a flat index with L2 distance)\n",
    "        index.add(all_embeddings)\n",
    "        faiss.write_index(index, index_path)\n",
    "        print(\"FAISS index initialized and saved.\")\n",
    "\n",
    "# 执行创建和保存数据 (Execute the creation and saving of data)\n",
    "create_and_save_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
